{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to obtain the Hugging face model, that is built in call_api, giving it the same parameters as the simplest of calls (single niah for mistral). Then, I create a new jsonl file containing the prompt, the answer, and both. Tokenization is non deterministic so test on multiple samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcfa07d522684437ae46c9199ed857ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mloscratch/homes/easselin/conda/envs/env/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from model_wrappers import HuggingFaceModel\n",
    "model_name_or_path=\"LargeWorldModel/LWM-Text-Chat-1M\"\n",
    "temperature=0.0\n",
    "top_k=1\n",
    "top_p=1.0\n",
    "stop_words=\"\"\n",
    "tokens_to_generate=128\n",
    "#We load the LLM\n",
    "llm=HuggingFaceModel(\n",
    "    name_or_path=model_name_or_path,\n",
    "    do_sample=temperature > 0,\n",
    "    repetition_penalty=1,\n",
    "    temperature=temperature,\n",
    "    top_k=top_k,\n",
    "    top_p=top_p,\n",
    "    stop=stop_words,\n",
    "    max_new_tokens=tokens_to_generate,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For debugging and comprehension purposes, this cell is to run call_api.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-07-18 14:04:00 nemo_logging:349] /mloscratch/homes/easselin/conda/envs/env/lib/python3.10/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "      warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict niah_single_1 \n",
      "from ../../results/mistralai/Mistral-7B-Instruct-v0.2/synthetic/4096/data/niah_single_1/validation.jsonl\n",
      "to ../../results/mistralai/Mistral-7B-Instruct-v0.2/synthetic/4096/pred/niah_single_1.jsonl\n",
      "DATA:[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:11<00:00,  3.82s/it]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used time: 0.4 minutes\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "MODEL_NAME=\"LargeWorldModel/LWM-Text-Chat-1M\"\n",
    "ROOT_DIR=\"../../results\"  \n",
    "MODEL_DIR=\"../../models\"\n",
    "BENCHMARK=\"synthetic\"\n",
    "MAX_SEQ_LENGTH=\"4096\"\n",
    "RESULTS_DIR=\"${ROOT_DIR}/${MODEL_NAME}/${BENCHMARK}/${MAX_SEQ_LENGTH}\"\n",
    "DATA_DIR=\"${RESULTS_DIR}/data\"\n",
    "PRED_DIR=\"${RESULTS_DIR}/pred\"\n",
    "TASK=\"niah_single_1\"\n",
    "MODEL_FRAMEWORK=\"hf\"\n",
    "MODEL_PATH=$MODEL_NAME\n",
    "TEMPERATURE=\"0.0\"\n",
    "TOP_P=\"1.0\"\n",
    "TOP_K=\"1\"\n",
    " python call_api.py \\\n",
    "            --data_dir ${DATA_DIR} \\\n",
    "            --save_dir ${PRED_DIR} \\\n",
    "            --benchmark ${BENCHMARK} \\\n",
    "            --task ${TASK} \\\n",
    "            --server_type ${MODEL_FRAMEWORK} \\\n",
    "            --model_name_or_path ${MODEL_PATH} \\\n",
    "            --temperature ${TEMPERATURE} \\\n",
    "            --top_k ${TOP_K} \\\n",
    "            --top_p ${TOP_P} \\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.asr.parts.utils.manifest_utils import read_manifest\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "task='niah_single_1'\n",
    "contextlength=16384\n",
    "strcontextlength=str(contextlength)\n",
    "example_path=\"examples/\"+model_name_or_path+\"/synthetic/\"+strcontextlength+\"/example_\"+task+\".jsonl\"\n",
    "data=read_manifest(example_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:07<00:00, 68.03it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(data))):\n",
    "    variables=data[i]\n",
    "    prompt=variables[\"prompt\"]\n",
    "    answer=variables[\"answer\"]\n",
    "    concat=variables[\"concatenation\"]\n",
    "    # assert prompt+answer==concat\n",
    "    prompttokenization=llm.tokenizer(prompt,return_tensors=\"pt\").input_ids\n",
    "    answertokenization=llm.tokenizer(answer,return_tensors=\"pt\",add_special_tokens=False).input_ids\n",
    "    concattokenization=llm.tokenizer(concat,return_tensors=\"pt\").input_ids\n",
    "    # print(f\"Prompt:       {' '.join(map(str, prompttokenization.tolist()))}\")\n",
    "    # print(f\"Answer:       {' '.join(map(str, answertokenization.tolist()))}\")\n",
    "    # print(f\"Concatenation:{' '.join(map(str, concattokenization.tolist()))}\")\n",
    "    # print(len(answertokenization[0].tolist()))\n",
    "    assert torch.allclose(torch.cat([prompttokenization,answertokenization],1),concattokenization)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If we don't remove the special tokens on the answer\n",
    "\n",
    "We can observe that if we remove the first two tokens of the answer's tokenization, then the concatenation of the tokenization\n",
    "of the prompt and the tokenization of the answer is the same as the tokenization of the concatenation of the prompt and the answer.\n",
    "\n",
    "### If we remove the special tokens on the answer\n",
    "\n",
    "#### If we put no spaces \n",
    "\n",
    "```\n",
    "-End of prompt:       \"...,2245, 28804]\"\n",
    "-Beginning of answer: \"[                 415, 2841, 9693,...\"\n",
    "-Transition in concat:\"...,2245, 28804, 1014, 2841, 9693,...\"\n",
    "```\n",
    "\n",
    "We can observe that here the first token of the answer differs between the two cases.\n",
    "\n",
    "#### If we put spaces\n",
    "\n",
    "```\n",
    "-End of prompt:       \"...,2245, 28804]\"\n",
    "-Beginning of answer: \"[                28705, 415, 2841, 9693,...\"\n",
    "-Transition in concat:\"...,2245, 28804,        415, 2841, 9693,...\"\n",
    "```\n",
    "\n",
    "We can observe that here there is an additional token at the beginning of the answer, even though we deactivated the special tokens (including BOS).\n",
    "\n",
    "#### If we put double spaces\n",
    "\n",
    "```\n",
    "-End of prompt:       \"...,2245, 28804]\"\n",
    "-Beginning of answer: \"[                  259, 415, 2841, 9693,...\"\n",
    "-Transition in concat:\"...,2245, 28804, 28705, 415, 2841, 9693,...\"\n",
    "```\n",
    "\n",
    "There is again a difference in the first token of the answer in the two cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity\n",
    "Here we implement perplexity evaluation for different configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [10:55<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average perplexity over the answer by passing the last 15000 tokens of the concatenation of the prompt and the answer as context:1.1903263330459595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 42/500 [00:58<10:38,  1.39s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 39\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m#----------------------------------------\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Evaluating perplexity when context does not include prompt --> average perplexity over the 500 samples is approximately 175\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# When context includes prompt, average perplexity is approximately 1.42\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# labels = answertokens.clone()\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# tokens = answertokens\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m#---------------------------------\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 39\u001b[0m     outputs\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     42\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss)\n",
      "File \u001b[0;32m/mloscratch/homes/easselin/conda/envs/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mloscratch/homes/easselin/conda/envs/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mloscratch/homes/easselin/conda/envs/env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1174\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1171\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1174\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1179\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1185\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1187\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/mloscratch/homes/easselin/conda/envs/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mloscratch/homes/easselin/conda/envs/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mloscratch/homes/easselin/conda/envs/env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:978\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    967\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    968\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    969\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    975\u001b[0m         cache_position,\n\u001b[1;32m    976\u001b[0m     )\n\u001b[1;32m    977\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 978\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    986\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    988\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    990\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/mloscratch/homes/easselin/conda/envs/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mloscratch/homes/easselin/conda/envs/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mloscratch/homes/easselin/conda/envs/env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:718\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    715\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    717\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 718\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    727\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    729\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m/mloscratch/homes/easselin/conda/envs/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mloscratch/homes/easselin/conda/envs/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mloscratch/homes/easselin/conda/envs/env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:425\u001b[0m, in \u001b[0;36mLlamaFlashAttention2.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[1;32m    422\u001b[0m key_states \u001b[38;5;241m=\u001b[39m key_states\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_key_value_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    423\u001b[0m value_states \u001b[38;5;241m=\u001b[39m value_states\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_key_value_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 425\u001b[0m cos, sin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrotary_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    426\u001b[0m query_states, key_states \u001b[38;5;241m=\u001b[39m apply_rotary_pos_emb(query_states, key_states, cos, sin)\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;66;03m# sin and cos are specific to RoPE models; cache_position needed for the static cache\u001b[39;00m\n",
      "File \u001b[0;32m/mloscratch/homes/easselin/conda/envs/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mloscratch/homes/easselin/conda/envs/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mloscratch/homes/easselin/conda/envs/env/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mloscratch/homes/easselin/conda/envs/env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:119\u001b[0m, in \u001b[0;36mLlamaRotaryEmbedding.forward\u001b[0;34m(self, x, position_ids)\u001b[0m\n\u001b[1;32m    117\u001b[0m     emb \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((freqs, freqs), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    118\u001b[0m     cos \u001b[38;5;241m=\u001b[39m emb\u001b[38;5;241m.\u001b[39mcos()\n\u001b[0;32m--> 119\u001b[0m     sin \u001b[38;5;241m=\u001b[39m \u001b[43memb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cos\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdtype), sin\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "losses=[]\n",
    "model=llm.pipeline.model if llm.pipeline else llm.model\n",
    "tokenizer=llm.pipeline.tokenizer if llm.pipeline else llm.tokenizer\n",
    "device=model.device\n",
    "stride=1000\n",
    "maxtokens=contextlength\n",
    "initnkeeplast=stride\n",
    "for nkeeplast in range(initnkeeplast*15,maxtokens,stride):\n",
    "    for sample in tqdm(data):\n",
    "\n",
    "        prompt=sample[\"prompt\"]\n",
    "        answer=sample[\"answer\"]\n",
    "        \n",
    "        \n",
    "        \n",
    "        prompttokens=tokenizer(prompt,return_tensors=\"pt\").to(device).input_ids\n",
    "        answertokens=tokenizer(answer,return_tensors=\"pt\",add_special_tokens=False).to(device).input_ids\n",
    "        concattokens=torch.cat([prompttokens,answertokens],1)\n",
    "        lengthconcat=len(concattokens[0].tolist())\n",
    "        \n",
    "        minnkeeplast=min(nkeeplast,lengthconcat)\n",
    "        tokens = concattokens[:,-minnkeeplast:]\n",
    "        labels = tokens.clone()\n",
    "        \n",
    "        \n",
    "        \n",
    "        lengthofanswer=len(answertokens[0].tolist())\n",
    "        labels[:,:-lengthofanswer]=-100\n",
    "\n",
    "        #----------------------------------------\n",
    "        # Evaluating perplexity when context does not include prompt --> average perplexity over the 500 samples is approximately 175\n",
    "        # When context includes prompt, average perplexity is approximately 1.42\n",
    "        # labels = answertokens.clone()\n",
    "        # tokens = answertokens\n",
    "        #---------------------------------\n",
    "        \n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs=model(tokens, labels=labels)\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        losses.append(loss)\n",
    "        \n",
    "    ppl=torch.exp(torch.stack(losses).mean())\n",
    "    print(f\"Average perplexity over the answer by passing the last {nkeeplast} tokens of the concatenation of the prompt and the answer as context:{ppl}\")\n",
    "    # print(f\"Average perplexity:{ppl}\")\n",
    "    losses.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Needle in the haystack task 1\n",
    "### Mistral 7B\n",
    "#### Max context length of 4096\n",
    "\n",
    "Average perplexity over the answer by passing only the answer as context:174.9510040283203  \n",
    "Average perplexity over the answer by passing the last 250 tokens of the concatenation of the prompt and the answer as context:3.442931890487671  \n",
    "Average perplexity over the answer by passing the last 500 tokens of the concatenation of the prompt and the answer as context:3.0682711601257324  \n",
    "Average perplexity over the answer by passing the last 750 tokens of the concatenation of the prompt and the answer as context:2.8815793991088867  \n",
    "Average perplexity over the answer by passing the last 1000 tokens of the concatenation of the prompt and the answer as context:2.6554274559020996  \n",
    "Average perplexity over the answer by passing the last 1250 tokens of the concatenation of the prompt and the answer as context:2.4824306964874268  \n",
    "Average perplexity over the answer by passing the last 1500 tokens of the concatenation of the prompt and the answer as context:2.3085646629333496  \n",
    "Average perplexity over the answer by passing the last 1750 tokens of the concatenation of the prompt and the answer as context:2.1747803688049316  \n",
    "Average perplexity over the answer by passing the last 2000 tokens of the concatenation of the prompt and the answer as context:2.0302517414093018  \n",
    "Average perplexity over the answer by passing the last 2250 tokens of the concatenation of the prompt and the answer as context:1.8966240882873535  \n",
    "Average perplexity over the answer by passing the last 2500 tokens of the concatenation of the prompt and the answer as context:1.7730286121368408  \n",
    "Average perplexity over the answer by passing the last 2750 tokens of the concatenation of the prompt and the answer as context:1.6829378604888916  \n",
    "Average perplexity over the answer by passing the last 3000 tokens of the concatenation of the prompt and the answer as context:1.5539112091064453  \n",
    "Average perplexity over the answer by passing the last 3250 tokens of the concatenation of the prompt and the answer as context:1.445020079612732  \n",
    "Average perplexity over the answer by passing the last 3500 tokens of the concatenation of the prompt and the answer as context:1.3325660228729248  \n",
    "Average perplexity over the answer by passing the last 3750 tokens of the concatenation of the prompt and the answer as context:1.2605961561203003  \n",
    "Average perplexity over the answer by passing all the tokens of the concatenation of the prompt and the answer as context:1.4215729236602783  \n",
    "\n",
    "#### Max context length of 8192\n",
    "\n",
    "Average perplexity over the answer by passing only the answer as context:175.12060546875  \n",
    "Average perplexity over the answer by passing the last 500 tokens of the concatenation of the prompt and the answer as context:3.25126051902771  \n",
    "Average perplexity over the answer by passing the last 1000 tokens of the concatenation of the prompt and the answer as context:2.9953126907348633  \n",
    "Average perplexity over the answer by passing the last 1500 tokens of the concatenation of the prompt and the answer as context:2.8163704872131348  \n",
    "Average perplexity over the answer by passing the last 2000 tokens of the concatenation of the prompt and the answer as context:2.563474655151367  \n",
    "Average perplexity over the answer by passing the last 2500 tokens of the concatenation of the prompt and the answer as context:2.3883209228515625  \n",
    "Average perplexity over the answer by passing the last 3000 tokens of the concatenation of the prompt and the answer as context:2.251896858215332  \n",
    "Average perplexity over the answer by passing the last 3500 tokens of the concatenation of the prompt and the answer as context:2.1382880210876465  \n",
    "Average perplexity over the answer by passing the last 4000 tokens of the concatenation of the prompt and the answer as context:2.0110976696014404  \n",
    "Average perplexity over the answer by passing the last 4500 tokens of the concatenation of the prompt and the answer as context:1.8779411315917969  \n",
    "Average perplexity over the answer by passing the last 5000 tokens of the concatenation of the prompt and the answer as context:1.7482768297195435  \n",
    "Average perplexity over the answer by passing the last 5500 tokens of the concatenation of the prompt and the answer as context:1.6497514247894287  \n",
    "Average perplexity over the answer by passing the last 6000 tokens of the concatenation of the prompt and the answer as context:1.525417447090149  \n",
    "Average perplexity over the answer by passing the last 6500 tokens of the concatenation of the prompt and the answer as context:1.4379204511642456  \n",
    "Average perplexity over the answer by passing the last 7000 tokens of the concatenation of the prompt and the answer as context:1.3026238679885864  \n",
    "Average perplexity over the answer by passing the last 7500 tokens of the concatenation of the prompt and the answer as context:1.2399232387542725  \n",
    "Average perplexity over the answer by passing all the tokens of the concatenation of the prompt and the answer as context:1.3250515460968018  \n",
    "\n",
    "Less perplexity for 8192 might be due to the fact that the model was trained to have context length 32k tokens?\n",
    "\n",
    "#### Max context length of 16384\n",
    "\n",
    "Average perplexity over the answer by passing only the answer as context:176.14071655273438  \n",
    "Average perplexity over the answer by passing the last 1000 tokens of the concatenation of the prompt and the answer as context:3.2502217292785645  \n",
    "Average perplexity over the answer by passing the last 2000 tokens of the concatenation of the prompt and the answer as context:2.955977439880371  \n",
    "Average perplexity over the answer by passing the last 3000 tokens of the concatenation of the prompt and the answer as context:2.786891460418701  \n",
    "Average perplexity over the answer by passing the last 4000 tokens of the concatenation of the prompt and the answer as context:2.5911900997161865  \n",
    "Average perplexity over the answer by passing the last 5000 tokens of the concatenation of the prompt and the answer as context:2.435096502304077  \n",
    "Average perplexity over the answer by passing the last 6000 tokens of the concatenation of the prompt and the answer as context:2.2886531352996826  \n",
    "Average perplexity over the answer by passing the last 7000 tokens of the concatenation of the prompt and the answer as context:2.172229766845703  \n",
    "Average perplexity over the answer by passing the last 8000 tokens of the concatenation of the prompt and the answer as context:2.0517468452453613  \n",
    "Average perplexity over the answer by passing the last 9000 tokens of the concatenation of the prompt and the answer as context:1.886850357055664  \n",
    "Average perplexity over the answer by passing the last 10000 tokens of the concatenation of the prompt and the answer as context:1.7527867555618286  \n",
    "Average perplexity over the answer by passing the last 11000 tokens of the concatenation of the prompt and the answer as context:1.6625641584396362  \n",
    "Average perplexity over the answer by passing the last 12000 tokens of the concatenation of the prompt and the answer as context:1.5579078197479248  \n",
    "Average perplexity over the answer by passing the last 13000 tokens of the concatenation of the prompt and the answer as context:1.45879065990448  \n",
    "Average perplexity over the answer by passing the last 14000 tokens of the concatenation of the prompt and the answer as context:1.3523118495941162  \n",
    "Average perplexity over the answer by passing the last 15000 tokens of the concatenation of the prompt and the answer as context:1.2737163305282593  \n",
    "Average perplexity over the answer by passing all the tokens of the concatenation of the prompt and the answer as context:1.307148814201355  \n",
    "\n",
    "### LWM\n",
    "\n",
    "#### Max context length of 4096\n",
    "\n",
    "Average perplexity over the answer by passing only the answer as context:78.059326171875  \n",
    "Average perplexity over the answer by passing the last 250 tokens of the concatenation of the prompt and the answer as context:2.7902402877807617  \n",
    "Average perplexity over the answer by passing the last 500 tokens of the concatenation of the prompt and the answer as context:2.6105449199676514  \n",
    "Average perplexity over the answer by passing the last 750 tokens of the concatenation of the prompt and the answer as context:2.483823776245117  \n",
    "Average perplexity over the answer by passing the last 1000 tokens of the concatenation of the prompt and the answer as context:2.3147709369659424  \n",
    "Average perplexity over the answer by passing the last 1250 tokens of the concatenation of the prompt and the answer as context:2.172853708267212  \n",
    "Average perplexity over the answer by passing the last 1500 tokens of the concatenation of the prompt and the answer as context:2.0439436435699463  \n",
    "Average perplexity over the answer by passing the last 1750 tokens of the concatenation of the prompt and the answer as context:1.943989634513855  \n",
    "Average perplexity over the answer by passing the last 2000 tokens of the concatenation of the prompt and the answer as context:1.8406829833984375  \n",
    "Average perplexity over the answer by passing the last 2250 tokens of the concatenation of the prompt and the answer as context:1.7273703813552856  \n",
    "Average perplexity over the answer by passing the last 2500 tokens of the concatenation of the prompt and the answer as context:1.6065398454666138  \n",
    "Average perplexity over the answer by passing the last 2750 tokens of the concatenation of the prompt and the answer as context:1.5346611738204956  \n",
    "Average perplexity over the answer by passing the last 3000 tokens of the concatenation of the prompt and the answer as context:1.4086531400680542  \n",
    "Average perplexity over the answer by passing the last 3250 tokens of the concatenation of the prompt and the answer as context:1.32203209400177  \n",
    "Average perplexity over the answer by passing the last 3500 tokens of the concatenation of the prompt and the answer as context:1.2222048044204712  \n",
    "Average perplexity over the answer by passing the last 3750 tokens of the concatenation of the prompt and the answer as context:1.1569379568099976  \n",
    "Average perplexity over the answer by passing all the tokens of the concatenation of the prompt and the answer as context:1.1488076448440552  \n",
    "\n",
    "\n",
    "#### Max context length of 8192\n",
    "\n",
    "Average perplexity over the answer by passing only the answer as context:78.05083465576172  \n",
    "Average perplexity over the answer by passing the last 500 tokens of the concatenation of the prompt and the answer as context:2.7583587169647217  \n",
    "Average perplexity over the answer by passing the last 1000 tokens of the concatenation of the prompt and the answer as context:2.5906922817230225  \n",
    "Average perplexity over the answer by passing the last 1500 tokens of the concatenation of the prompt and the answer as context:2.4765517711639404  \n",
    "Average perplexity over the answer by passing the last 2000 tokens of the concatenation of the prompt and the answer as context:2.304326057434082  \n",
    "Average perplexity over the answer by passing the last 2500 tokens of the concatenation of the prompt and the answer as context:2.1398379802703857  \n",
    "Average perplexity over the answer by passing the last 3000 tokens of the concatenation of the prompt and the answer as context:2.0182175636291504  \n",
    "Average perplexity over the answer by passing the last 3500 tokens of the concatenation of the prompt and the answer as context:1.9385747909545898  \n",
    "Average perplexity over the answer by passing the last 4000 tokens of the concatenation of the prompt and the answer as context:1.8298826217651367  \n",
    "Average perplexity over the answer by passing the last 4500 tokens of the concatenation of the prompt and the answer as context:1.7096558809280396  \n",
    "Average perplexity over the answer by passing the last 5000 tokens of the concatenation of the prompt and the answer as context:1.6005035638809204  \n",
    "Average perplexity over the answer by passing the last 5500 tokens of the concatenation of the prompt and the answer as context:1.5128042697906494  \n",
    "Average perplexity over the answer by passing the last 6000 tokens of the concatenation of the prompt and the answer as context:1.3970189094543457  \n",
    "Average perplexity over the answer by passing the last 6500 tokens of the concatenation of the prompt and the answer as context:1.3171504735946655  \n",
    "Average perplexity over the answer by passing the last 7000 tokens of the concatenation of the prompt and the answer as context:1.2068501710891724  \n",
    "Average perplexity over the answer by passing the last 7500 tokens of the concatenation of the prompt and the answer as context:1.151483416557312    \n",
    "Average perplexity over the answer by passing all the tokens of the concatenation of the prompt and the answer as context:1.1486250162124634  \n",
    "\n",
    "#### Max context length of 16384\n",
    "\n",
    "Average perplexity over the answer by passing only the answer as context:78.03204345703125  \n",
    "Average perplexity over the answer by passing the last 1000 tokens of the concatenation of the prompt and the answer as context:2.793048858642578  \n",
    "Average perplexity over the answer by passing the last 2000 tokens of the concatenation of the prompt and the answer as context:2.635413408279419  \n",
    "Average perplexity over the answer by passing the last 3000 tokens of the concatenation of the prompt and the answer as context:2.4776053428649902  \n",
    "Average perplexity over the answer by passing the last 4000 tokens of the concatenation of the prompt and the answer as context:2.3439559936523438  \n",
    "Average perplexity over the answer by passing the last 5000 tokens of the concatenation of the prompt and the answer as context:2.202911853790283  \n",
    "Average perplexity over the answer by passing the last 6000 tokens of the concatenation of the prompt and the answer as context:2.0741262435913086  \n",
    "Average perplexity over the answer by passing the last 7000 tokens of the concatenation of the prompt and the answer as context:1.981422781944275  \n",
    "Average perplexity over the answer by passing the last 8000 tokens of the concatenation of the prompt and the answer as context:1.884514570236206  \n",
    "Average perplexity over the answer by passing the last 9000 tokens of the concatenation of the prompt and the answer as context:1.746207594871521  \n",
    "Average perplexity over the answer by passing the last 10000 tokens of the concatenation of the prompt and the answer as context:1.6279515027999878    \n",
    "Average perplexity over the answer by passing the last 11000 tokens of the concatenation of the prompt and the answer as context:1.5399411916732788  \n",
    "Average perplexity over the answer by passing the last 12000 tokens of the concatenation of the prompt and the answer as context:1.4471083879470825  \n",
    "Average perplexity over the answer by passing the last 13000 tokens of the concatenation of the prompt and the answer as context:1.3545198440551758  \n",
    "Average perplexity over the answer by passing the last 14000 tokens of the concatenation of the prompt and the answer as context:1.2605664730072021   \n",
    "Average perplexity over the answer by passing the last 15000 tokens of the concatenation of the prompt and the answer as context:1.1903263330459595   \n",
    "Average perplexity over the answer by passing all the tokens of the concatenation of the prompt and the asnwer as context:1.1593486070632935\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
