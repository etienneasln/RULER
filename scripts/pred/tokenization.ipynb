{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to obtain the Hugging face model, that is built in call_api, giving it the same parameters as the simplest of calls (single niah for mistral). Then, I create a new jsonl file containing the prompt, the answer, and both. Tokenization is non deterministic so test multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebe5c07f6cbb4fc99a03a35a1608486c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from model_wrappers import HuggingFaceModel\n",
    "model_name_or_path=\"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "temperature=0.0\n",
    "top_k=1\n",
    "top_p=1.0\n",
    "stop_words=\"\"\n",
    "tokens_to_generate=128\n",
    "#We load the LLM\n",
    "llm=HuggingFaceModel(\n",
    "    name_or_path=model_name_or_path,\n",
    "    do_sample=temperature > 0,\n",
    "    repetition_penalty=1,\n",
    "    temperature=temperature,\n",
    "    top_k=top_k,\n",
    "    top_p=top_p,\n",
    "    stop=stop_words,\n",
    "    max_new_tokens=tokens_to_generate,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For debugging and comprehension purposes, this cell is to run call_api.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-07-18 14:04:00 nemo_logging:349] /mloscratch/homes/easselin/conda/envs/env/lib/python3.10/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "      warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict niah_single_1 \n",
      "from ../../results/mistralai/Mistral-7B-Instruct-v0.2/synthetic/4096/data/niah_single_1/validation.jsonl\n",
      "to ../../results/mistralai/Mistral-7B-Instruct-v0.2/synthetic/4096/pred/niah_single_1.jsonl\n",
      "DATA:[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:11<00:00,  3.82s/it]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used time: 0.4 minutes\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "MODEL_NAME=\"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "ROOT_DIR=\"../../results\"  \n",
    "MODEL_DIR=\"../../models\"\n",
    "BENCHMARK=\"synthetic\"\n",
    "MAX_SEQ_LENGTH=\"4096\"\n",
    "RESULTS_DIR=\"${ROOT_DIR}/${MODEL_NAME}/${BENCHMARK}/${MAX_SEQ_LENGTH}\"\n",
    "DATA_DIR=\"${RESULTS_DIR}/data\"\n",
    "PRED_DIR=\"${RESULTS_DIR}/pred\"\n",
    "TASK=\"niah_single_1\"\n",
    "MODEL_FRAMEWORK=\"hf\"\n",
    "MODEL_PATH=$MODEL_NAME\n",
    "TEMPERATURE=\"0.0\"\n",
    "TOP_P=\"1.0\"\n",
    "TOP_K=\"1\"\n",
    " python call_api.py \\\n",
    "            --data_dir ${DATA_DIR} \\\n",
    "            --save_dir ${PRED_DIR} \\\n",
    "            --benchmark ${BENCHMARK} \\\n",
    "            --task ${TASK} \\\n",
    "            --server_type ${MODEL_FRAMEWORK} \\\n",
    "            --model_name_or_path ${MODEL_PATH} \\\n",
    "            --temperature ${TEMPERATURE} \\\n",
    "            --top_k ${TOP_K} \\\n",
    "            --top_p ${TOP_P} \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-07-22 08:41:26 nemo_logging:349] /mloscratch/homes/easselin/conda/envs/env/lib/python3.10/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "      warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n",
      "    \n",
      "100%|██████████| 500/500 [00:06<00:00, 79.06it/s]\n"
     ]
    }
   ],
   "source": [
    "from nemo.collections.asr.parts.utils.manifest_utils import read_manifest\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "example_path=\"example.jsonl\"\n",
    "data=read_manifest(example_path)\n",
    "for i in tqdm(range(len(data))):\n",
    "    variables=data[i]\n",
    "    prompt=variables[\"prompt\"]\n",
    "    answer=variables[\"answer\"]\n",
    "    concat=variables[\"concatenation\"]\n",
    "    # assert prompt+answer==concat\n",
    "    prompttokenization=llm.tokenizer(prompt,return_tensors=\"pt\").input_ids\n",
    "    answertokenization=llm.tokenizer(answer,return_tensors=\"pt\",add_special_tokens=False).input_ids\n",
    "    concattokenization=llm.tokenizer(concat,return_tensors=\"pt\").input_ids\n",
    "    # print(f\"Prompt:       {' '.join(map(str, prompttokenization.tolist()))}\")\n",
    "    # print(f\"Answer:       {' '.join(map(str, answertokenization.tolist()))}\")\n",
    "    # print(f\"Concatenation:{' '.join(map(str, concattokenization.tolist()))}\")\n",
    "    assert torch.allclose(torch.cat([prompttokenization,answertokenization],1),concattokenization)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If we don't remove the special tokens on the answer\n",
    "\n",
    "We can observe that if we remove the first two tokens of the answer's tokenization, then the concatenation of the tokenization\n",
    "of the prompt and the tokenization of the answer is the same as the tokenization of the concatenation of the prompt and the answer.\n",
    "\n",
    "### If we remove the special tokens on the answer\n",
    "\n",
    "#### If we put no spaces \n",
    "\n",
    "```\n",
    "-End of prompt:       \"...,2245, 28804]\"\n",
    "-Beginning of answer: \"[                 415, 2841, 9693,...\"\n",
    "-Transition in concat:\"...,2245, 28804, 1014, 2841, 9693,...\"\n",
    "```\n",
    "\n",
    "We can observe that here the first token of the answer differs between the two cases.\n",
    "\n",
    "#### If we put spaces\n",
    "\n",
    "```\n",
    "-End of prompt:       \"...,2245, 28804]\"\n",
    "-Beginning of answer: \"[                28705, 415, 2841, 9693,...\"\n",
    "-Transition in concat:\"...,2245, 28804,        415, 2841, 9693,...\"\n",
    "```\n",
    "\n",
    "We can observe that here there is an additional token at the beginning of the answer, even though we deactivated the special tokens (including BOS).\n",
    "\n",
    "#### If we put double spaces\n",
    "\n",
    "```\n",
    "-End of prompt:       \"...,2245, 28804]\"\n",
    "-Beginning of answer: \"[                  259, 415, 2841, 9693,...\"\n",
    "-Transition in concat:\"...,2245, 28804, 28705, 415, 2841, 9693,...\"\n",
    "```\n",
    "\n",
    "There is again a difference in the first token of the answer in the two cases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
