{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to obtain the Hugging face model, that is built in call_api, giving it the same parameters as the simplest of calls (single niah for mistral). Then, I create a new jsonl file containing the prompt, the answer, and both. Tokenization is non deterministic so test on multiple samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cdc7290ed424f9aa04aa4a03541aa0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-07-30 08:41:44 nemo_logging:349] /mloscratch/homes/easselin/conda/envs/env/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2024-07-30 08:41:44 nemo_logging:349] /mloscratch/homes/easselin/conda/envs/env/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2024-07-30 08:41:44 nemo_logging:349] /mloscratch/homes/easselin/conda/envs/env/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2024-07-30 08:41:44 nemo_logging:349] /mloscratch/homes/easselin/conda/envs/env/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "      warnings.warn(\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from model_wrappers import HuggingFaceModel\n",
    "model_name_or_path=\"lmsys/longchat-7b-v1.5-32k\"\n",
    "temperature=0.0\n",
    "top_k=1\n",
    "top_p=1.0\n",
    "stop_words=\"\"\n",
    "tokens_to_generate=128\n",
    "#We load the LLM\n",
    "llm=HuggingFaceModel(\n",
    "    name_or_path=model_name_or_path,\n",
    "    do_sample=temperature > 0,\n",
    "    repetition_penalty=1,\n",
    "    temperature=temperature,\n",
    "    top_k=top_k,\n",
    "    top_p=top_p,\n",
    "    stop=stop_words,\n",
    "    max_new_tokens=tokens_to_generate,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For debugging and comprehension purposes, this cell is to run call_api.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process is interrupted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-07-26 14:20:33 nemo_logging:349] /mloscratch/homes/easselin/conda/envs/env/lib/python3.10/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "      warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict niah_single_1 \n",
      "from ../../results/lmsys/longchat-7b-v1.5-32k/synthetic/16384/data/niah_single_1/validation.jsonl\n",
      "to ../../results/lmsys/longchat-7b-v1.5-32k/synthetic/16384/pred/niah_single_1.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s][NeMo W 2024-07-26 14:20:47 nemo_logging:349] /mloscratch/homes/easselin/conda/envs/env/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "      return self.fget.__get__(instance, owner)()\n",
      "    \n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.45s/it]\n",
      "[NeMo W 2024-07-26 14:20:54 nemo_logging:349] /mloscratch/homes/easselin/conda/envs/env/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2024-07-26 14:20:54 nemo_logging:349] /mloscratch/homes/easselin/conda/envs/env/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2024-07-26 14:20:54 nemo_logging:349] /mloscratch/homes/easselin/conda/envs/env/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2024-07-26 14:20:54 nemo_logging:349] /mloscratch/homes/easselin/conda/envs/env/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "      warnings.warn(\n",
      "    \n",
      "  0%|          | 0/500 [00:00<?, ?it/s][NeMo W 2024-07-26 14:20:55 nemo_logging:349] /mloscratch/homes/easselin/conda/envs/env/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2024-07-26 14:20:55 nemo_logging:349] /mloscratch/homes/easselin/conda/envs/env/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:562: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "      warnings.warn(\n",
      "    \n",
      "Traceback (most recent call last):\n",
      "  File \"/mloscratch/homes/easselin/sil-internship/RULER/scripts/pred/call_api.py\", line 254, in get_output\n",
      "    pred_list = llm.process_batch(prompts=input_list)\n",
      "  File \"/mloscratch/homes/easselin/sil-internship/RULER/scripts/pred/model_wrappers.py\", line 69, in process_batch\n",
      "    output = self.pipeline(text_inputs=prompts, **self.generation_kwargs, )\n",
      "  File \"/mloscratch/homes/easselin/conda/envs/env/lib/python3.10/site-packages/transformers/pipelines/text_generation.py\", line 262, in __call__\n",
      "    return super().__call__(text_inputs, **kwargs)\n",
      "  File \"/mloscratch/homes/easselin/conda/envs/env/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1235, in __call__\n",
      "    outputs = list(final_iterator)\n",
      "  File \"/mloscratch/homes/easselin/conda/envs/env/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py\", line 124, in __next__\n",
      "    item = next(self.iterator)\n",
      "  File \"/mloscratch/homes/easselin/conda/envs/env/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py\", line 125, in __next__\n",
      "    processed = self.infer(item, **self.params)\n",
      "  File \"/mloscratch/homes/easselin/conda/envs/env/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1161, in forward\n",
      "    model_outputs = self._forward(model_inputs, **forward_params)\n",
      "  File \"/mloscratch/homes/easselin/conda/envs/env/lib/python3.10/site-packages/transformers/pipelines/text_generation.py\", line 349, in _forward\n",
      "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
      "  File \"/mloscratch/homes/easselin/conda/envs/env/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/mloscratch/homes/easselin/conda/envs/env/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1914, in generate\n",
      "    result = self._sample(\n",
      "  File \"/mloscratch/homes/easselin/conda/envs/env/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2651, in _sample\n",
      "    outputs = self(\n",
      "  File \"/mloscratch/homes/easselin/conda/envs/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/mloscratch/homes/easselin/conda/envs/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/mloscratch/homes/easselin/conda/envs/env/lib/python3.10/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "  File \"/mloscratch/homes/easselin/conda/envs/env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 1174, in forward\n",
      "    outputs = self.model(\n",
      "  File \"/mloscratch/homes/easselin/conda/envs/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/mloscratch/homes/easselin/conda/envs/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/mloscratch/homes/easselin/conda/envs/env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 978, in forward\n",
      "    layer_outputs = decoder_layer(\n",
      "  File \"/mloscratch/homes/easselin/conda/envs/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/mloscratch/homes/easselin/conda/envs/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/mloscratch/homes/easselin/conda/envs/env/lib/python3.10/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "  File \"/mloscratch/homes/easselin/conda/envs/env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 732, in forward\n",
      "    hidden_states = self.mlp(hidden_states)\n",
      "  File \"/mloscratch/homes/easselin/conda/envs/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/mloscratch/homes/easselin/conda/envs/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/mloscratch/homes/easselin/conda/envs/env/lib/python3.10/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "  File \"/mloscratch/homes/easselin/conda/envs/env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 215, in forward\n",
      "    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 330.00 MiB. GPU 1 has a total capacty of 79.15 GiB of which 291.44 MiB is free. Process 120291 has 16.51 GiB memory in use. Process 121886 has 16.51 GiB memory in use. Process 122220 has 16.51 GiB memory in use. Process 128157 has 16.51 GiB memory in use. Process 131512 has 12.82 GiB memory in use. Of the allocated memory 10.88 GiB is allocated by PyTorch, and 1.43 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "MODEL_NAME=\"lmsys/longchat-7b-v1.5-32k\"\n",
    "ROOT_DIR=\"../../results\"  \n",
    "MODEL_DIR=\"../../models\"\n",
    "BENCHMARK=\"synthetic\"\n",
    "MAX_SEQ_LENGTH=\"16384\"\n",
    "RESULTS_DIR=\"${ROOT_DIR}/${MODEL_NAME}/${BENCHMARK}/${MAX_SEQ_LENGTH}\"\n",
    "DATA_DIR=\"${RESULTS_DIR}/data\"\n",
    "PRED_DIR=\"${RESULTS_DIR}/pred\"\n",
    "TASK=\"niah_single_1\"\n",
    "MODEL_FRAMEWORK=\"hf\"\n",
    "MODEL_PATH=$MODEL_NAME\n",
    "TEMPERATURE=\"0.0\"\n",
    "TOP_P=\"1.0\"\n",
    "TOP_K=\"1\"\n",
    " python call_api.py \\\n",
    "            --data_dir ${DATA_DIR} \\\n",
    "            --save_dir ${PRED_DIR} \\\n",
    "            --benchmark ${BENCHMARK} \\\n",
    "            --task ${TASK} \\\n",
    "            --server_type ${MODEL_FRAMEWORK} \\\n",
    "            --model_name_or_path ${MODEL_PATH} \\\n",
    "            --temperature ${TEMPERATURE} \\\n",
    "            --top_k ${TOP_K} \\\n",
    "            --top_p ${TOP_P} \\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.asr.parts.utils.manifest_utils import read_manifest\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "task='niah_single_1'\n",
    "contextlength=4096\n",
    "strcontextlength=str(contextlength)\n",
    "example_path=\"examples/\"+model_name_or_path+\"/synthetic/\"+strcontextlength+\"/example_\"+task+\".jsonl\"\n",
    "data=read_manifest(example_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:05<00:00, 83.65it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(data))):\n",
    "    variables=data[i]\n",
    "    prompt=variables[\"prompt\"]\n",
    "    answer=variables[\"answer\"]\n",
    "    concat=variables[\"concatenation\"]\n",
    "    # assert prompt+answer==concat\n",
    "    prompttokenization=llm.tokenizer(prompt,return_tensors=\"pt\").input_ids\n",
    "    answertokenization=llm.tokenizer(answer,return_tensors=\"pt\",add_special_tokens=False).input_ids\n",
    "    concattokenization=llm.tokenizer(concat,return_tensors=\"pt\").input_ids\n",
    "    # print(f\"Prompt:       {' '.join(map(str, prompttokenization.tolist()))}\")\n",
    "    # print(f\"Answer:       {' '.join(map(str, answertokenization.tolist()))}\")\n",
    "    # print(f\"Concatenation:{' '.join(map(str, concattokenization.tolist()))}\")\n",
    "    # print(len(answertokenization[0].tolist()))\n",
    "    assert torch.allclose(torch.cat([prompttokenization,answertokenization],1),concattokenization)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If we don't remove the special tokens on the answer\n",
    "\n",
    "We can observe that if we remove the first two tokens of the answer's tokenization, then the concatenation of the tokenization\n",
    "of the prompt and the tokenization of the answer is the same as the tokenization of the concatenation of the prompt and the answer.\n",
    "\n",
    "### If we remove the special tokens on the answer\n",
    "\n",
    "#### If we put no spaces \n",
    "\n",
    "```\n",
    "-End of prompt:       \"...,2245, 28804]\"\n",
    "-Beginning of answer: \"[                 415, 2841, 9693,...\"\n",
    "-Transition in concat:\"...,2245, 28804, 1014, 2841, 9693,...\"\n",
    "```\n",
    "\n",
    "We can observe that here the first token of the answer differs between the two cases.\n",
    "\n",
    "#### If we put spaces\n",
    "\n",
    "```\n",
    "-End of prompt:       \"...,2245, 28804]\"\n",
    "-Beginning of answer: \"[                28705, 415, 2841, 9693,...\"\n",
    "-Transition in concat:\"...,2245, 28804,        415, 2841, 9693,...\"\n",
    "```\n",
    "\n",
    "We can observe that here there is an additional token at the beginning of the answer, even though we deactivated the special tokens (including BOS).\n",
    "\n",
    "#### If we put double spaces\n",
    "\n",
    "```\n",
    "-End of prompt:       \"...,2245, 28804]\"\n",
    "-Beginning of answer: \"[                  259, 415, 2841, 9693,...\"\n",
    "-Transition in concat:\"...,2245, 28804, 28705, 415, 2841, 9693,...\"\n",
    "```\n",
    "\n",
    "There is again a difference in the first token of the answer in the two cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity\n",
    "Here we implement perplexity evaluation for different configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "100%|██████████| 500/500 [00:21<00:00, 23.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average perplexity over the answer by passing the last 250 tokens of the concatenation of the prompt and the answer as context:3.520951747894287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:23<00:00, 21.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average perplexity over the answer by passing the last 500 tokens of the concatenation of the prompt and the answer as context:3.2366271018981934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:30<00:00, 16.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average perplexity over the answer by passing the last 750 tokens of the concatenation of the prompt and the answer as context:3.0646657943725586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:44<00:00, 11.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average perplexity over the answer by passing the last 1000 tokens of the concatenation of the prompt and the answer as context:2.8605432510375977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:48<00:00, 10.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average perplexity over the answer by passing the last 1250 tokens of the concatenation of the prompt and the answer as context:2.676612377166748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:57<00:00,  8.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average perplexity over the answer by passing the last 1500 tokens of the concatenation of the prompt and the answer as context:2.4917852878570557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:12<00:00,  6.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average perplexity over the answer by passing the last 1750 tokens of the concatenation of the prompt and the answer as context:2.354177236557007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:19<00:00,  6.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average perplexity over the answer by passing the last 2000 tokens of the concatenation of the prompt and the answer as context:2.2453253269195557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:27<00:00,  5.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average perplexity over the answer by passing the last 2250 tokens of the concatenation of the prompt and the answer as context:2.0984466075897217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:31<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average perplexity over the answer by passing the last 2500 tokens of the concatenation of the prompt and the answer as context:1.9636598825454712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:48<00:00,  4.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average perplexity over the answer by passing the last 2750 tokens of the concatenation of the prompt and the answer as context:1.8619531393051147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:55<00:00,  4.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average perplexity over the answer by passing the last 3000 tokens of the concatenation of the prompt and the answer as context:1.7078495025634766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:02<00:00,  4.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average perplexity over the answer by passing the last 3250 tokens of the concatenation of the prompt and the answer as context:1.6058282852172852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:19<00:00,  3.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average perplexity over the answer by passing the last 3500 tokens of the concatenation of the prompt and the answer as context:1.4817001819610596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:23<00:00,  3.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average perplexity over the answer by passing the last 3750 tokens of the concatenation of the prompt and the answer as context:1.403790831565857\n"
     ]
    }
   ],
   "source": [
    "losses=[]\n",
    "model=llm.pipeline.model if llm.pipeline else llm.model\n",
    "tokenizer=llm.pipeline.tokenizer if llm.pipeline else llm.tokenizer\n",
    "device=model.device\n",
    "stride=250\n",
    "maxtokens=contextlength-stride\n",
    "initnkeeplast=stride\n",
    "for nkeeplast in range(initnkeeplast,maxtokens,stride):\n",
    "    for sample in tqdm(data):\n",
    "\n",
    "        prompt=sample[\"prompt\"]\n",
    "        answer=sample[\"answer\"]\n",
    "        \n",
    "        \n",
    "        \n",
    "        prompttokens=tokenizer(prompt,return_tensors=\"pt\").to(device).input_ids\n",
    "        answertokens=tokenizer(answer,return_tensors=\"pt\",add_special_tokens=False).to(device).input_ids\n",
    "        concattokens=torch.cat([prompttokens,answertokens],1)\n",
    "        lengthconcat=len(concattokens[0].tolist())\n",
    "        \n",
    "        tokens = concattokens[:,-nkeeplast:]\n",
    "        labels = tokens.clone()\n",
    "        \n",
    "        \n",
    "        \n",
    "        lengthofanswer=len(answertokens[0].tolist())\n",
    "        labels[:,:-lengthofanswer]=-100\n",
    "\n",
    "        \n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs=model(tokens, labels=labels)\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        losses.append(loss)\n",
    "        \n",
    "    ppl=torch.exp(torch.stack(losses).mean())\n",
    "    print(f\"Average perplexity over the answer by passing the last {nkeeplast} tokens of the concatenation of the prompt and the answer as context:{ppl}\")\n",
    "    # print(f\"Average perplexity:{ppl}\")\n",
    "    losses.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Needle in the haystack task 1\n",
    "### Mistral 7B\n",
    "#### Max context length of 4096\n",
    "\n",
    "Average perplexity over the answer by passing only the answer as context:174.9510040283203  \n",
    "Average perplexity over the answer by passing the last 250 tokens of the concatenation of the prompt and the answer as context:3.442931890487671  \n",
    "Average perplexity over the answer by passing the last 500 tokens of the concatenation of the prompt and the answer as context:3.0682711601257324  \n",
    "Average perplexity over the answer by passing the last 750 tokens of the concatenation of the prompt and the answer as context:2.8815793991088867  \n",
    "Average perplexity over the answer by passing the last 1000 tokens of the concatenation of the prompt and the answer as context:2.6554274559020996  \n",
    "Average perplexity over the answer by passing the last 1250 tokens of the concatenation of the prompt and the answer as context:2.4824306964874268  \n",
    "Average perplexity over the answer by passing the last 1500 tokens of the concatenation of the prompt and the answer as context:2.3085646629333496  \n",
    "Average perplexity over the answer by passing the last 1750 tokens of the concatenation of the prompt and the answer as context:2.1747803688049316  \n",
    "Average perplexity over the answer by passing the last 2000 tokens of the concatenation of the prompt and the answer as context:2.0302517414093018  \n",
    "Average perplexity over the answer by passing the last 2250 tokens of the concatenation of the prompt and the answer as context:1.8966240882873535  \n",
    "Average perplexity over the answer by passing the last 2500 tokens of the concatenation of the prompt and the answer as context:1.7730286121368408  \n",
    "Average perplexity over the answer by passing the last 2750 tokens of the concatenation of the prompt and the answer as context:1.6829378604888916  \n",
    "Average perplexity over the answer by passing the last 3000 tokens of the concatenation of the prompt and the answer as context:1.5539112091064453  \n",
    "Average perplexity over the answer by passing the last 3250 tokens of the concatenation of the prompt and the answer as context:1.445020079612732  \n",
    "Average perplexity over the answer by passing the last 3500 tokens of the concatenation of the prompt and the answer as context:1.3325660228729248  \n",
    "Average perplexity over the answer by passing the last 3750 tokens of the concatenation of the prompt and the answer as context:1.2605961561203003  \n",
    "Average perplexity over the answer by passing all the tokens of the concatenation of the prompt and the answer as context:1.4215729236602783  \n",
    "\n",
    "#### Max context length of 8192\n",
    "\n",
    "Average perplexity over the answer by passing only the answer as context:175.12060546875  \n",
    "Average perplexity over the answer by passing the last 500 tokens of the concatenation of the prompt and the answer as context:3.25126051902771  \n",
    "Average perplexity over the answer by passing the last 1000 tokens of the concatenation of the prompt and the answer as context:2.9953126907348633  \n",
    "Average perplexity over the answer by passing the last 1500 tokens of the concatenation of the prompt and the answer as context:2.8163704872131348  \n",
    "Average perplexity over the answer by passing the last 2000 tokens of the concatenation of the prompt and the answer as context:2.563474655151367  \n",
    "Average perplexity over the answer by passing the last 2500 tokens of the concatenation of the prompt and the answer as context:2.3883209228515625  \n",
    "Average perplexity over the answer by passing the last 3000 tokens of the concatenation of the prompt and the answer as context:2.251896858215332  \n",
    "Average perplexity over the answer by passing the last 3500 tokens of the concatenation of the prompt and the answer as context:2.1382880210876465  \n",
    "Average perplexity over the answer by passing the last 4000 tokens of the concatenation of the prompt and the answer as context:2.0110976696014404  \n",
    "Average perplexity over the answer by passing the last 4500 tokens of the concatenation of the prompt and the answer as context:1.8779411315917969  \n",
    "Average perplexity over the answer by passing the last 5000 tokens of the concatenation of the prompt and the answer as context:1.7482768297195435  \n",
    "Average perplexity over the answer by passing the last 5500 tokens of the concatenation of the prompt and the answer as context:1.6497514247894287  \n",
    "Average perplexity over the answer by passing the last 6000 tokens of the concatenation of the prompt and the answer as context:1.525417447090149  \n",
    "Average perplexity over the answer by passing the last 6500 tokens of the concatenation of the prompt and the answer as context:1.4379204511642456  \n",
    "Average perplexity over the answer by passing the last 7000 tokens of the concatenation of the prompt and the answer as context:1.3026238679885864  \n",
    "Average perplexity over the answer by passing the last 7500 tokens of the concatenation of the prompt and the answer as context:1.2399232387542725  \n",
    "Average perplexity over the answer by passing all the tokens of the concatenation of the prompt and the answer as context:1.3250515460968018  \n",
    "\n",
    "Less perplexity for 8192 might be due to the fact that the model was trained to have context length 32k tokens?\n",
    "\n",
    "#### Max context length of 16384\n",
    "\n",
    "Average perplexity over the answer by passing only the answer as context:176.14071655273438  \n",
    "Average perplexity over the answer by passing the last 1000 tokens of the concatenation of the prompt and the answer as context:3.2502217292785645  \n",
    "Average perplexity over the answer by passing the last 2000 tokens of the concatenation of the prompt and the answer as context:2.955977439880371  \n",
    "Average perplexity over the answer by passing the last 3000 tokens of the concatenation of the prompt and the answer as context:2.786891460418701  \n",
    "Average perplexity over the answer by passing the last 4000 tokens of the concatenation of the prompt and the answer as context:2.5911900997161865  \n",
    "Average perplexity over the answer by passing the last 5000 tokens of the concatenation of the prompt and the answer as context:2.435096502304077  \n",
    "Average perplexity over the answer by passing the last 6000 tokens of the concatenation of the prompt and the answer as context:2.2886531352996826  \n",
    "Average perplexity over the answer by passing the last 7000 tokens of the concatenation of the prompt and the answer as context:2.172229766845703  \n",
    "Average perplexity over the answer by passing the last 8000 tokens of the concatenation of the prompt and the answer as context:2.0517468452453613  \n",
    "Average perplexity over the answer by passing the last 9000 tokens of the concatenation of the prompt and the answer as context:1.886850357055664  \n",
    "Average perplexity over the answer by passing the last 10000 tokens of the concatenation of the prompt and the answer as context:1.7527867555618286  \n",
    "Average perplexity over the answer by passing the last 11000 tokens of the concatenation of the prompt and the answer as context:1.6625641584396362  \n",
    "Average perplexity over the answer by passing the last 12000 tokens of the concatenation of the prompt and the answer as context:1.5579078197479248  \n",
    "Average perplexity over the answer by passing the last 13000 tokens of the concatenation of the prompt and the answer as context:1.45879065990448  \n",
    "Average perplexity over the answer by passing the last 14000 tokens of the concatenation of the prompt and the answer as context:1.3523118495941162  \n",
    "Average perplexity over the answer by passing the last 15000 tokens of the concatenation of the prompt and the answer as context:1.2737163305282593  \n",
    "Average perplexity over the answer by passing all the tokens of the concatenation of the prompt and the answer as context:1.307148814201355  \n",
    "\n",
    "### LWM\n",
    "\n",
    "#### Max context length of 4096\n",
    "\n",
    "Average perplexity over the answer by passing only the answer as context:78.059326171875  \n",
    "Average perplexity over the answer by passing the last 250 tokens of the concatenation of the prompt and the answer as context:2.7902402877807617  \n",
    "Average perplexity over the answer by passing the last 500 tokens of the concatenation of the prompt and the answer as context:2.6105449199676514  \n",
    "Average perplexity over the answer by passing the last 750 tokens of the concatenation of the prompt and the answer as context:2.483823776245117  \n",
    "Average perplexity over the answer by passing the last 1000 tokens of the concatenation of the prompt and the answer as context:2.3147709369659424  \n",
    "Average perplexity over the answer by passing the last 1250 tokens of the concatenation of the prompt and the answer as context:2.172853708267212  \n",
    "Average perplexity over the answer by passing the last 1500 tokens of the concatenation of the prompt and the answer as context:2.0439436435699463  \n",
    "Average perplexity over the answer by passing the last 1750 tokens of the concatenation of the prompt and the answer as context:1.943989634513855  \n",
    "Average perplexity over the answer by passing the last 2000 tokens of the concatenation of the prompt and the answer as context:1.8406829833984375  \n",
    "Average perplexity over the answer by passing the last 2250 tokens of the concatenation of the prompt and the answer as context:1.7273703813552856  \n",
    "Average perplexity over the answer by passing the last 2500 tokens of the concatenation of the prompt and the answer as context:1.6065398454666138  \n",
    "Average perplexity over the answer by passing the last 2750 tokens of the concatenation of the prompt and the answer as context:1.5346611738204956  \n",
    "Average perplexity over the answer by passing the last 3000 tokens of the concatenation of the prompt and the answer as context:1.4086531400680542  \n",
    "Average perplexity over the answer by passing the last 3250 tokens of the concatenation of the prompt and the answer as context:1.32203209400177  \n",
    "Average perplexity over the answer by passing the last 3500 tokens of the concatenation of the prompt and the answer as context:1.2222048044204712  \n",
    "Average perplexity over the answer by passing the last 3750 tokens of the concatenation of the prompt and the answer as context:1.1569379568099976  \n",
    "Average perplexity over the answer by passing all the tokens of the concatenation of the prompt and the answer as context:1.1488076448440552  \n",
    "\n",
    "\n",
    "#### Max context length of 8192\n",
    "\n",
    "Average perplexity over the answer by passing only the answer as context:78.05083465576172  \n",
    "Average perplexity over the answer by passing the last 500 tokens of the concatenation of the prompt and the answer as context:2.7583587169647217  \n",
    "Average perplexity over the answer by passing the last 1000 tokens of the concatenation of the prompt and the answer as context:2.5906922817230225  \n",
    "Average perplexity over the answer by passing the last 1500 tokens of the concatenation of the prompt and the answer as context:2.4765517711639404  \n",
    "Average perplexity over the answer by passing the last 2000 tokens of the concatenation of the prompt and the answer as context:2.304326057434082  \n",
    "Average perplexity over the answer by passing the last 2500 tokens of the concatenation of the prompt and the answer as context:2.1398379802703857  \n",
    "Average perplexity over the answer by passing the last 3000 tokens of the concatenation of the prompt and the answer as context:2.0182175636291504  \n",
    "Average perplexity over the answer by passing the last 3500 tokens of the concatenation of the prompt and the answer as context:1.9385747909545898  \n",
    "Average perplexity over the answer by passing the last 4000 tokens of the concatenation of the prompt and the answer as context:1.8298826217651367  \n",
    "Average perplexity over the answer by passing the last 4500 tokens of the concatenation of the prompt and the answer as context:1.7096558809280396  \n",
    "Average perplexity over the answer by passing the last 5000 tokens of the concatenation of the prompt and the answer as context:1.6005035638809204  \n",
    "Average perplexity over the answer by passing the last 5500 tokens of the concatenation of the prompt and the answer as context:1.5128042697906494  \n",
    "Average perplexity over the answer by passing the last 6000 tokens of the concatenation of the prompt and the answer as context:1.3970189094543457  \n",
    "Average perplexity over the answer by passing the last 6500 tokens of the concatenation of the prompt and the answer as context:1.3171504735946655  \n",
    "Average perplexity over the answer by passing the last 7000 tokens of the concatenation of the prompt and the answer as context:1.2068501710891724  \n",
    "Average perplexity over the answer by passing the last 7500 tokens of the concatenation of the prompt and the answer as context:1.151483416557312    \n",
    "Average perplexity over the answer by passing all the tokens of the concatenation of the prompt and the answer as context:1.1486250162124634  \n",
    "\n",
    "#### Max context length of 16384\n",
    "\n",
    "Average perplexity over the answer by passing only the answer as context:78.03204345703125  \n",
    "Average perplexity over the answer by passing the last 1000 tokens of the concatenation of the prompt and the answer as context:2.793048858642578  \n",
    "Average perplexity over the answer by passing the last 2000 tokens of the concatenation of the prompt and the answer as context:2.635413408279419  \n",
    "Average perplexity over the answer by passing the last 3000 tokens of the concatenation of the prompt and the answer as context:2.4776053428649902  \n",
    "Average perplexity over the answer by passing the last 4000 tokens of the concatenation of the prompt and the answer as context:2.3439559936523438  \n",
    "Average perplexity over the answer by passing the last 5000 tokens of the concatenation of the prompt and the answer as context:2.202911853790283  \n",
    "Average perplexity over the answer by passing the last 6000 tokens of the concatenation of the prompt and the answer as context:2.0741262435913086  \n",
    "Average perplexity over the answer by passing the last 7000 tokens of the concatenation of the prompt and the answer as context:1.981422781944275  \n",
    "Average perplexity over the answer by passing the last 8000 tokens of the concatenation of the prompt and the answer as context:1.884514570236206  \n",
    "Average perplexity over the answer by passing the last 9000 tokens of the concatenation of the prompt and the answer as context:1.746207594871521  \n",
    "Average perplexity over the answer by passing the last 10000 tokens of the concatenation of the prompt and the answer as context:1.6279515027999878    \n",
    "Average perplexity over the answer by passing the last 11000 tokens of the concatenation of the prompt and the answer as context:1.5399411916732788  \n",
    "Average perplexity over the answer by passing the last 12000 tokens of the concatenation of the prompt and the answer as context:1.4471083879470825  \n",
    "Average perplexity over the answer by passing the last 13000 tokens of the concatenation of the prompt and the answer as context:1.3545198440551758  \n",
    "Average perplexity over the answer by passing the last 14000 tokens of the concatenation of the prompt and the answer as context:1.2605664730072021   \n",
    "Average perplexity over the answer by passing the last 15000 tokens of the concatenation of the prompt and the answer as context:1.1903263330459595   \n",
    "Average perplexity over the answer by passing all the tokens of the concatenation of the prompt and the asnwer as context:1.1593486070632935\n",
    "\n",
    "### LongChat\n",
    "\n",
    "#### Max context length of 4096\n",
    "\n",
    "Average perplexity over the answer by passing only the answer as context:84.53561401367188  \n",
    "Average perplexity over the answer by passing the last 250 tokens of the concatenation of the prompt and the answer as context:3.520951747894287  \n",
    "Average perplexity over the answer by passing the last 500 tokens of the concatenation of the prompt and the answer as context:3.2366271018981934  \n",
    "Average perplexity over the answer by passing the last 750 tokens of the concatenation of the prompt and the answer as context:3.0646657943725586  \n",
    "Average perplexity over the answer by passing the last 1000 tokens of the concatenation of the prompt and the answer as context:2.8605432510375977  \n",
    "Average perplexity over the answer by passing the last 1250 tokens of the concatenation of the prompt and the answer as context:2.676612377166748  \n",
    "Average perplexity over the answer by passing the last 1500 tokens of the concatenation of the prompt and the answer as context:2.4917852878570557  \n",
    "Average perplexity over the answer by passing the last 1750 tokens of the concatenation of the prompt and the answer as context:2.354177236557007  \n",
    "Average perplexity over the answer by passing the last 2000 tokens of the concatenation of the prompt and the answer as context:2.2453253269195557  \n",
    "Average perplexity over the answer by passing the last 2250 tokens of the concatenation of the prompt and the answer as context:2.0984466075897217  \n",
    "Average perplexity over the answer by passing the last 2500 tokens of the concatenation of the prompt and the answer as context:1.9636598825454712  \n",
    "Average perplexity over the answer by passing the last 2750 tokens of the concatenation of the prompt and the answer as context:1.8619531393051147  \n",
    "Average perplexity over the answer by passing the last 3000 tokens of the concatenation of the prompt and the answer as context:1.7078495025634766  \n",
    "Average perplexity over the answer by passing the last 3250 tokens of the concatenation of the prompt and the answer as context:1.6058282852172852  \n",
    "Average perplexity over the answer by passing the last 3500 tokens of the concatenation of the prompt and the answer as context:1.4817001819610596  \n",
    "Average perplexity over the answer by passing the last 3750 tokens of the concatenation of the prompt and the answer as context:1.403790831565857  \n",
    "Average perplexity over the answer by passing all the tokens of the concatenation of the prompt and the answer as context:1.626822590827942  \n",
    "\n",
    "#### Max context length of 8192\n",
    "\n",
    "Average perplexity over the answer by passing only the answer as context:84.63575744628906  \n",
    "Average perplexity over the answer by passing the last 500 tokens of the concatenation of the prompt and the answer as context:3.4246301651000977  \n",
    "Average perplexity over the answer by passing the last 1000 tokens of the concatenation of the prompt and the answer as context:3.199336051940918   \n",
    "Average perplexity over the answer by passing the last 1500 tokens of the concatenation of the prompt and the answer as context:3.0150558948516846  \n",
    "Average perplexity over the answer by passing the last 2000 tokens of the concatenation of the prompt and the answer as context:2.8123795986175537  \n",
    "Average perplexity over the answer by passing the last 2500 tokens of the concatenation of the prompt and the answer as context:2.599802017211914  \n",
    "Average perplexity over the answer by passing the last 3000 tokens of the concatenation of the prompt and the answer as context:2.4217867851257324  \n",
    "Average perplexity over the answer by passing the last 3500 tokens of the concatenation of the prompt and the answer as context:2.3292667865753174  \n",
    "Average perplexity over the answer by passing the last 4000 tokens of the concatenation of the prompt and the answer as context:2.175149917602539  \n",
    "Average perplexity over the answer by passing the last 4500 tokens of the concatenation of the prompt and the answer as context:2.036252975463867  \n",
    "Average perplexity over the answer by passing the last 5000 tokens of the concatenation of the prompt and the answer as context:1.9171148538589478  \n",
    "Average perplexity over the answer by passing the last 5500 tokens of the concatenation of the prompt and the answer as context:1.8126466274261475  \n",
    "Average perplexity over the answer by passing the last 6000 tokens of the concatenation of the prompt and the answer as context:1.6782187223434448  \n",
    "Average perplexity over the answer by passing the last 6500 tokens of the concatenation of the prompt and the answer as context:1.5899064540863037  \n",
    "Average perplexity over the answer by passing the last 7000 tokens of the concatenation of the prompt and the answer as context:1.4453948736190796  \n",
    "Average perplexity over the answer by passing the last 7500 tokens of the concatenation of the prompt and the answer as context:1.3939419984817505  \n",
    "Average perplexity over the answer by passing all the tokens of the concatenation of the prompt and the answer as context:1.6423969268798828\n",
    "\n",
    "#### Max context length of 16384\n",
    "\n",
    "Average perplexity over the answer by passing only the answer as context:84.92120361328125  \n",
    "Average perplexity over the answer by passing the last 1000 tokens of the concatenation of the prompt and the answer as context:3.449686050415039  \n",
    "Average perplexity over the answer by passing the last 2000 tokens of the concatenation of the prompt and the answer as context:3.2106401920318604  \n",
    "Average perplexity over the answer by passing the last 3000 tokens of the concatenation of the prompt and the answer as context:2.953179359436035  \n",
    "Average perplexity over the answer by passing the last 4000 tokens of the concatenation of the prompt and the answer as context:2.772296667098999  \n",
    "Average perplexity over the answer by passing the last 5000 tokens of the concatenation of the prompt and the answer as context:2.618698835372925  \n",
    "Average perplexity over the answer by passing the last 6000 tokens of the concatenation of the prompt and the answer as context:2.453761577606201  \n",
    "Average perplexity over the answer by passing the last 7000 tokens of the concatenation of the prompt and the answer as context:2.3301615715026855  \n",
    "Average perplexity over the answer by passing the last 8000 tokens of the concatenation of the prompt and the answer as context:2.231750011444092  \n",
    "Average perplexity over the answer by passing the last 9000 tokens of the concatenation of the prompt and the answer as context:2.0827157497406006  \n",
    "Average perplexity over the answer by passing the last 10000 tokens of the concatenation of the prompt and the answer as context:1.9356590509414673  \n",
    "Average perplexity over the answer by passing the last 11000 tokens of the concatenation of the prompt and the answer as context:1.8202787637710571  \n",
    "Average perplexity over the answer by passing the last 12000 tokens of the concatenation of the prompt and the answer as context:1.7264333963394165  \n",
    "Average perplexity over the answer by passing the last 13000 tokens of the concatenation of the prompt and the answer as context:1.6375830173492432  \n",
    "Average perplexity over the answer by passing the last 14000 tokens of the concatenation of the prompt and the answer as context:1.5031508207321167  \n",
    "Average perplexity over the answer by passing the last 15000 tokens of the concatenation of the prompt and the answer as context:1.4270881414413452   \n",
    "Average perplexity over the answer by passing all the tokens of the concatenation of the prompt and the answer as context:1.5213795900344849 \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
